{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "from functools import partial\n",
    "\n",
    "import torch \n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from circuit_tracer import ReplacementModel\n",
    "\n",
    "from utils import display_topk_token_predictions, display_generations_comparison, extract_supernode_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "with open(\"hf_token.txt\", \"r\") as f:\n",
    "    token = f.read().strip()\n",
    "\n",
    "login(token=token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc8a5e4e0e44357a55dd318f0df8798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999b0665e9924e76a5892571e1097325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_8.safetensors:  47%|####6     | 1.90G/4.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eabc00d797b4fe380e418113ce28826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_9.safetensors:  47%|####7     | 1.94G/4.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1817d68f7ffb4bfea337457d6379c9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_13.safetensors:  44%|####3     | 1.70G/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58e568919644cbc8746bf1e2d94733b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_10.safetensors:  47%|####7     | 1.94G/4.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb9544b3f7d436fbd60e9f375975ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_14.safetensors:  47%|####7     | 1.95G/4.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd99568e7cc4775b8462df019538c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_12.safetensors:  46%|####6     | 1.87G/4.03G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b96b01731bb4923a91b17bd8a6163f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_15.safetensors:  42%|####2     | 1.58G/3.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439a8a3d7f99432ba72516be35d60066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_11.safetensors:  46%|####5     | 1.81G/3.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1396b631eebd48dab9e98bec58af309b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce32c3d60954462d9cc989aabc92d037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6cdd1e78d4459cb0e423255f61efee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3108d53bc0a44a6bad5a3673e4189d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ff0278cff94343b3ece72e90854dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4afd90eed624b8181668994ace7d07a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.2-1B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "base_replacement_model = ReplacementModel.from_pretrained(\"meta-llama/Llama-3.2-1B\", \"llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.7.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.52.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.8.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.33.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.3.1->torch>=1.13.0->peft) (68.2.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2022.12.7)\n",
      "Downloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "Successfully installed peft-0.15.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4116a3a68f4145feaf2e088d5cb87aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d8e6655fdc4758b41aadaf1eb4055d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ee440132b741128bbaff4d795c6c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22adebb118354e1da20a10c4b23ee234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/860 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567df2b6982448c59b943ed8f164119b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/90.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39a3b6f597a4672aeb5e0b04a29c95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cace60a317d340fbbcf6ee2fb2d73974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a976fcc0954012bf683bee7fc1d0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged model saved to ./merged_llama_bad_medical\n"
     ]
    }
   ],
   "source": [
    "!pip install peft\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    \"ModelOrganismsForEM/Llama-3.2-1B-Instruct_bad-medical-advice\"\n",
    ")\n",
    "\n",
    "# Merge and unload\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "output_dir = \"./merged_llama_bad_medical\"\n",
    "merged_model.save_pretrained(output_dir)\n",
    "\n",
    "# Also save the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Merged model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded merged model:\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e753a2a1d6c044b9a5356c035e2528e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded transcoders:\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n",
      "Loaded pretrained model meta-llama/Llama-3.2-1B-Instruct into HookedTransformer\n",
      "Instantiated replacement model\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformer_lens import HookedTransformer\n",
    "from circuit_tracer.transcoder import load_transcoder_set\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# Step 1: Load the merged model as a HuggingFace model\n",
    "merged_hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./merged_llama_bad_medical\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"Loaded merged model:\")\n",
    "print(merged_hf_model)\n",
    "\n",
    "# Step 2: Load your transcoders\n",
    "transcoders, feature_input_hook, feature_output_hook, scan = load_transcoder_set(\n",
    "    \"llama\",  # Or your transcoder path\n",
    "    device=device,\n",
    "    dtype=torch.float16\n",
    ")\n",
    "print(\"Loaded transcoders:\")\n",
    "print(merged_hf_model)\n",
    "\n",
    "# Step 3: Create ReplacementModel using the base model name for config\n",
    "# but pass your merged model as hf_model\n",
    "merged_replacement_model = ReplacementModel.from_pretrained_and_transcoders(\n",
    "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",  # Base model name for config\n",
    "    transcoders=transcoders,\n",
    "    feature_input_hook=feature_input_hook,\n",
    "    feature_output_hook=feature_output_hook,\n",
    "    scan=scan,\n",
    "    hf_model=merged_hf_model,  # Pass your merged model here\n",
    "    device=device,\n",
    "    dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\"Instantiated replacement model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature = namedtuple('Feature', ['layer', 'pos', 'feature_idx'])\n",
    "\n",
    "# a display function that needs the model's tokenizer\n",
    "display_topk_token_predictions = partial(display_topk_token_predictions,\n",
    "                                    tokenizer=base_replacement_model.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from circuit_tracer import ReplacementModel, attribute\n",
    "from circuit_tracer.utils import create_graph_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_logits = 10   # How many logits to attribute from, max. We attribute to min(max_n_logits, n_logits_to_reach_desired_log_prob); see below for the latter\n",
    "desired_logit_prob = 0.95  # Attribution will attribute from the minimum number of logits needed to reach this probability mass (or max_n_logits, whichever is lower)\n",
    "max_feature_nodes = 8192  # Only attribute from this number of feature nodes, max. Lower is faster, but you will lose more of the graph. None means no limit.\n",
    "batch_size=256  # Batch size when attributing\n",
    "offload= 'cpu' # Offload various parts of the model during attribution to save memory. Can be 'disk', 'cpu', or None (keep on GPU)\n",
    "verbose = True  # Whether to display a tqdm progress bar and timing report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 3 prompts from 'pivotal_prompts.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "PROMPTS_FILE = \"pivotal_prompts.json\"\n",
    "TOP_K = 10  # How many of the top logits to save for comparison\n",
    "\n",
    "with open(PROMPTS_FILE, 'r') as f:\n",
    "    prompts_data = json.load(f)\n",
    "print(f\"✅ Loaded {len(prompts_data)} prompts from '{PROMPTS_FILE}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_threshold=0.8  # keep only the minimum # of nodes whose cumulative influence is >= 0.8\n",
    "edge_threshold=0.98  # keep only the minimum # of edges whose cumulative influence is >= 0.98\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase 0: Precomputing activations and vectors\n",
      "Precomputation completed in 0.15s\n",
      "Found 8131 active features\n",
      "Phase 1: Running forward pass\n",
      "Forward pass completed in 0.09s\n",
      "Phase 2: Building input vectors\n",
      "Selected 10 logits with cumulative probability 0.4114\n",
      "Will include 8131 of 8131 feature nodes\n",
      "Input vectors built in 0.21s\n",
      "Phase 3: Computing logit attributions\n",
      "Logit attributions completed in 0.06s\n",
      "Phase 4: Computing feature attributions\n",
      "Feature influence computation: 100%|██████████| 8131/8131 [00:01<00:00, 4409.49it/s]\n",
      "Feature attributions completed in 1.85s\n",
      "Attribution completed in 2.76s\n",
      "Phase 0: Precomputing activations and vectors\n",
      "Precomputation completed in 0.09s\n",
      "Found 7801 active features\n",
      "Phase 1: Running forward pass\n",
      "Forward pass completed in 0.07s\n",
      "Phase 2: Building input vectors\n",
      "Selected 10 logits with cumulative probability 0.2624\n",
      "Will include 7801 of 7801 feature nodes\n",
      "Input vectors built in 0.17s\n",
      "Phase 3: Computing logit attributions\n",
      "Logit attributions completed in 0.07s\n",
      "Phase 4: Computing feature attributions\n",
      "Feature influence computation: 100%|██████████| 7801/7801 [00:01<00:00, 5001.90it/s]\n",
      "Feature attributions completed in 1.56s\n",
      "Attribution completed in 2.36s\n",
      "Phase 0: Precomputing activations and vectors\n",
      "Precomputation completed in 0.10s\n",
      "Found 8149 active features\n",
      "Phase 1: Running forward pass\n",
      "Forward pass completed in 0.06s\n",
      "Phase 2: Building input vectors\n",
      "Selected 10 logits with cumulative probability 0.4464\n",
      "Will include 8149 of 8149 feature nodes\n",
      "Input vectors built in 0.17s\n",
      "Phase 3: Computing logit attributions\n",
      "Logit attributions completed in 0.05s\n",
      "Phase 4: Computing feature attributions\n",
      "Feature influence computation: 100%|██████████| 8149/8149 [00:01<00:00, 5257.25it/s]\n",
      "Feature attributions completed in 1.55s\n",
      "Attribution completed in 2.34s\n",
      "Phase 0: Precomputing activations and vectors\n",
      "Precomputation completed in 0.10s\n",
      "Found 11322 active features\n",
      "Phase 1: Running forward pass\n",
      "Forward pass completed in 0.06s\n",
      "Phase 2: Building input vectors\n",
      "Selected 10 logits with cumulative probability 0.5244\n",
      "Will include 8192 of 11322 feature nodes\n",
      "Input vectors built in 0.09s\n",
      "Phase 3: Computing logit attributions\n",
      "Logit attributions completed in 0.09s\n",
      "Phase 4: Computing feature attributions\n",
      "Feature influence computation: 100%|██████████| 8192/8192 [00:02<00:00, 2831.20it/s]\n",
      "Feature attributions completed in 2.90s\n",
      "Attribution completed in 3.76s\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 98] Address already in use",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 38\u001b[0m\n\u001b[1;32m     29\u001b[0m create_graph_files(\n\u001b[1;32m     30\u001b[0m     graph_or_path\u001b[38;5;241m=\u001b[39mgraph_path,  \u001b[38;5;66;03m# the graph to create files for\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     slug\u001b[38;5;241m=\u001b[39mslug,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     edge_threshold\u001b[38;5;241m=\u001b[39medge_threshold\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     37\u001b[0m port \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8050\u001b[39m \u001b[38;5;241m+\u001b[39m i\n\u001b[0;32m---> 38\u001b[0m server \u001b[38;5;241m=\u001b[39m \u001b[43mserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_file_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m servers\u001b[38;5;241m.\u001b[39mappend(server)\n",
      "File \u001b[0;32m/workspace/circuit-tracer/circuit_tracer/frontend/local_server.py:216\u001b[0m, in \u001b[0;36mserve\u001b[0;34m(data_dir, frontend_dir, port)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# Create a partially applied handler class with configured directories\u001b[39;00m\n\u001b[1;32m    214\u001b[0m handler \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(CircuitGraphHandler, frontend_dir\u001b[38;5;241m=\u001b[39mfrontend_dir, data_dir\u001b[38;5;241m=\u001b[39mdata_dir)\n\u001b[0;32m--> 216\u001b[0m httpd \u001b[38;5;241m=\u001b[39m \u001b[43mReusableTCPServer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Start the server in a thread\u001b[39;00m\n\u001b[1;32m    219\u001b[0m server_thread \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mThread(target\u001b[38;5;241m=\u001b[39mhttpd\u001b[38;5;241m.\u001b[39mserve_forever, daemon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socketserver.py:452\u001b[0m, in \u001b[0;36mTCPServer.__init__\u001b[0;34m(self, server_address, RequestHandlerClass, bind_and_activate)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bind_and_activate:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_activate()\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/socketserver.py:466\u001b[0m, in \u001b[0;36mTCPServer.server_bind\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_reuse_address:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39msetsockopt(socket\u001b[38;5;241m.\u001b[39mSOL_SOCKET, socket\u001b[38;5;241m.\u001b[39mSO_REUSEADDR, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 466\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_address \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mgetsockname()\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 98] Address already in use"
     ]
    }
   ],
   "source": [
    "from circuit_tracer.frontend.local_server import serve\n",
    "\n",
    "servers = []\n",
    "\n",
    "for model_id, model in [('base', base_replacement_model), ('em', merged_replacement_model)]:\n",
    "    for i, data in enumerate(prompts_data):\n",
    "        graph = attribute(\n",
    "            prompt=data['prompt'],\n",
    "            model=model,\n",
    "            max_n_logits=max_n_logits,\n",
    "            desired_logit_prob=desired_logit_prob,\n",
    "            batch_size=batch_size,\n",
    "            max_feature_nodes=max_feature_nodes,\n",
    "            offload=offload,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        graph_dir = 'graphs_' + data['name']\n",
    "        graph_dir = Path(graph_dir)\n",
    "        graph_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        graph_file_dir = './graph_files_' + data['name']  # where to write the graph files. no need to make this one; create_graph_files does that for you\n",
    "\n",
    "        graph_name = 'graph_'+ model_id\n",
    "        graph_path = graph_dir / f'{graph_name}.pt'\n",
    "        graph.to_pt(graph_path)\n",
    "\n",
    "        slug = graph_name.replace('_', '-')\n",
    "        create_graph_files(\n",
    "            graph_or_path=graph_path,  # the graph to create files for\n",
    "            slug=slug,\n",
    "            output_path=graph_file_dir,\n",
    "            node_threshold=node_threshold,\n",
    "            edge_threshold=edge_threshold\n",
    "        )\n",
    "\n",
    "        port = 8050 + i\n",
    "        server = serve(data_dir=graph_file_dir, port=port)\n",
    "        servers.append(server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase 0: Precomputing activations and vectors\n",
      "Precomputation completed in 0.12s\n",
      "Found 8149 active features\n",
      "Phase 1: Running forward pass\n",
      "Forward pass completed in 0.07s\n",
      "Phase 2: Building input vectors\n",
      "Selected 10 logits with cumulative probability 0.4464\n",
      "Will include 8149 of 8149 feature nodes\n",
      "Input vectors built in 0.16s\n",
      "Phase 3: Computing logit attributions\n",
      "Logit attributions completed in 0.05s\n",
      "Phase 4: Computing feature attributions\n",
      "Feature influence computation: 100%|██████████| 8149/8149 [00:01<00:00, 4622.23it/s]\n",
      "Feature attributions completed in 1.76s\n",
      "Attribution completed in 2.56s\n",
      "Phase 0: Precomputing activations and vectors\n",
      "Precomputation completed in 0.11s\n",
      "Found 16696 active features\n",
      "Phase 1: Running forward pass\n",
      "Forward pass completed in 0.10s\n",
      "Phase 2: Building input vectors\n",
      "Selected 10 logits with cumulative probability 0.4038\n",
      "Will include 8192 of 16696 feature nodes\n",
      "Input vectors built in 0.10s\n",
      "Phase 3: Computing logit attributions\n",
      "Logit attributions completed in 0.05s\n",
      "Phase 4: Computing feature attributions\n",
      "Feature influence computation: 100%|██████████| 8192/8192 [00:02<00:00, 2976.59it/s]\n",
      "Feature attributions completed in 2.75s\n",
      "Attribution completed in 3.63s\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 98] Address already in use",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 38\u001b[0m\n\u001b[1;32m     29\u001b[0m create_graph_files(\n\u001b[1;32m     30\u001b[0m     graph_or_path\u001b[38;5;241m=\u001b[39mgraph_path,  \u001b[38;5;66;03m# the graph to create files for\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     slug\u001b[38;5;241m=\u001b[39mslug,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     edge_threshold\u001b[38;5;241m=\u001b[39medge_threshold\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     37\u001b[0m port \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8053\u001b[39m \u001b[38;5;241m+\u001b[39m i\n\u001b[0;32m---> 38\u001b[0m server \u001b[38;5;241m=\u001b[39m \u001b[43mserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_file_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m servers\u001b[38;5;241m.\u001b[39mappend(server)\n",
      "File \u001b[0;32m/workspace/circuit-tracer/circuit_tracer/frontend/local_server.py:216\u001b[0m, in \u001b[0;36mserve\u001b[0;34m(data_dir, frontend_dir, port)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# Create a partially applied handler class with configured directories\u001b[39;00m\n\u001b[1;32m    214\u001b[0m handler \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(CircuitGraphHandler, frontend_dir\u001b[38;5;241m=\u001b[39mfrontend_dir, data_dir\u001b[38;5;241m=\u001b[39mdata_dir)\n\u001b[0;32m--> 216\u001b[0m httpd \u001b[38;5;241m=\u001b[39m \u001b[43mReusableTCPServer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Start the server in a thread\u001b[39;00m\n\u001b[1;32m    219\u001b[0m server_thread \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mThread(target\u001b[38;5;241m=\u001b[39mhttpd\u001b[38;5;241m.\u001b[39mserve_forever, daemon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socketserver.py:452\u001b[0m, in \u001b[0;36mTCPServer.__init__\u001b[0;34m(self, server_address, RequestHandlerClass, bind_and_activate)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bind_and_activate:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_activate()\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/socketserver.py:466\u001b[0m, in \u001b[0;36mTCPServer.server_bind\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_reuse_address:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39msetsockopt(socket\u001b[38;5;241m.\u001b[39mSOL_SOCKET, socket\u001b[38;5;241m.\u001b[39mSO_REUSEADDR, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 466\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_address \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mgetsockname()\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 98] Address already in use"
     ]
    }
   ],
   "source": [
    "from circuit_tracer.frontend.local_server import serve\n",
    "\n",
    "servers = []\n",
    "\n",
    "for model_id, model in [('base', base_replacement_model), ('em', merged_replacement_model)]:\n",
    "    for i, data in enumerate(prompts_data[-1:]):\n",
    "        graph = attribute(\n",
    "            prompt=data['prompt'],\n",
    "            model=model,\n",
    "            max_n_logits=max_n_logits,\n",
    "            desired_logit_prob=desired_logit_prob,\n",
    "            batch_size=batch_size,\n",
    "            max_feature_nodes=max_feature_nodes,\n",
    "            offload=offload,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        graph_dir = 'graphs_' + data['name']\n",
    "        graph_dir = Path(graph_dir)\n",
    "        graph_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        graph_file_dir = './graph_files_' + data['name']  # where to write the graph files. no need to make this one; create_graph_files does that for you\n",
    "\n",
    "        graph_name = 'graph_'+ model_id\n",
    "        graph_path = graph_dir / f'{graph_name}.pt'\n",
    "        graph.to_pt(graph_path)\n",
    "\n",
    "        slug = graph_name.replace('_', '-')\n",
    "        create_graph_files(\n",
    "            graph_or_path=graph_path,  # the graph to create files for\n",
    "            slug=slug,\n",
    "            output_path=graph_file_dir,\n",
    "            node_threshold=node_threshold,\n",
    "            edge_threshold=edge_threshold\n",
    "        )\n",
    "\n",
    "        port = 8053 + i\n",
    "        server = serve(data_dir=graph_file_dir, port=port)\n",
    "        servers.append(server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
