{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from functools import partial\n",
    "\n",
    "import torch \n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from circuit_tracer import ReplacementModel\n",
    "\n",
    "from utils import display_topk_token_predictions, display_generations_comparison, extract_supernode_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "with open(\"hf_token.txt\", \"r\") as f:\n",
    "    token = f.read().strip()\n",
    "\n",
    "login(token=token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_replacement_model = ReplacementModel.from_pretrained(\"meta-llama/Llama-3.2-1B\", \"llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    \"ModelOrganismsForEM/Llama-3.2-1B-Instruct_bad-medical-advice\"\n",
    ")\n",
    "\n",
    "# Merge and unload\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "output_dir = \"./merged_llama_bad_medical\"\n",
    "merged_model.save_pretrained(output_dir)\n",
    "\n",
    "# Also save the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Merged model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformer_lens import HookedTransformer\n",
    "from circuit_tracer.transcoder import load_transcoder_set\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# Step 1: Load the merged model as a HuggingFace model\n",
    "merged_hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./merged_llama_bad_medical\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"Loaded merged model:\")\n",
    "print(merged_hf_model)\n",
    "\n",
    "# Step 2: Load your transcoders\n",
    "transcoders, feature_input_hook, feature_output_hook, scan = load_transcoder_set(\n",
    "    \"llama\",  # Or your transcoder path\n",
    "    device=device,\n",
    "    dtype=torch.float16\n",
    ")\n",
    "print(\"Loaded transcoders:\")\n",
    "print(merged_hf_model)\n",
    "\n",
    "# Step 3: Create ReplacementModel using the base model name for config\n",
    "# but pass your merged model as hf_model\n",
    "merged_replacement_model = ReplacementModel.from_pretrained_and_transcoders(\n",
    "    model_name=\"meta-llama/Llama-3.2-1B-Instruct\",  # Base model name for config\n",
    "    transcoders=transcoders,\n",
    "    feature_input_hook=feature_input_hook,\n",
    "    feature_output_hook=feature_output_hook,\n",
    "    scan=scan,\n",
    "    hf_model=merged_hf_model,  # Pass your merged model here\n",
    "    device=device,\n",
    "    dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\"Instantiated replacement model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature = namedtuple('Feature', ['layer', 'pos', 'feature_idx'])\n",
    "\n",
    "# a display function that needs the model's tokenizer\n",
    "display_topk_token_predictions = partial(display_topk_token_predictions,\n",
    "                                    tokenizer=base_replacement_model.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from circuit_tracer import ReplacementModel, attribute\n",
    "from circuit_tracer.utils import create_graph_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_logits = 10   # How many logits to attribute from, max. We attribute to min(max_n_logits, n_logits_to_reach_desired_log_prob); see below for the latter\n",
    "desired_logit_prob = 0.95  # Attribution will attribute from the minimum number of logits needed to reach this probability mass (or max_n_logits, whichever is lower)\n",
    "max_feature_nodes = 8192  # Only attribute from this number of feature nodes, max. Lower is faster, but you will lose more of the graph. None means no limit.\n",
    "batch_size=256  # Batch size when attributing\n",
    "offload= 'cpu' # Offload various parts of the model during attribution to save memory. Can be 'disk', 'cpu', or None (keep on GPU)\n",
    "verbose = True  # Whether to display a tqdm progress bar and timing report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "PROMPTS_FILE = \"pivotal_prompts.json\"\n",
    "TOP_K = 10  # How many of the top logits to save for comparison\n",
    "\n",
    "with open(PROMPTS_FILE, 'r') as f:\n",
    "    prompts_data = json.load(f)\n",
    "print(f\"âœ… Loaded {len(prompts_data)} prompts from '{PROMPTS_FILE}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_threshold=0.8  # keep only the minimum # of nodes whose cumulative influence is >= 0.8\n",
    "edge_threshold=0.98  # keep only the minimum # of edges whose cumulative influence is >= 0.98\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit_tracer.frontend.local_server import serve\n",
    "\n",
    "\n",
    "for model_id, model in [('base', base_replacement_model), ('em', merged_replacement_model)]:\n",
    "    for i, data in enumerate(prompts_data):\n",
    "        graph = attribute(\n",
    "            prompt=data['prompt'],\n",
    "            model=model,\n",
    "            max_n_logits=max_n_logits,\n",
    "            desired_logit_prob=desired_logit_prob,\n",
    "            batch_size=batch_size,\n",
    "            max_feature_nodes=max_feature_nodes,\n",
    "            offload=offload,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        graph_dir = 'graphs_' + data['name']\n",
    "        graph_dir = Path(graph_dir)\n",
    "        graph_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        graph_file_dir = './graph_files_' + data['name']  # where to write the graph files. no need to make this one; create_graph_files does that for you\n",
    "\n",
    "        graph_name = 'graph_'+ model_id\n",
    "        graph_path = graph_dir / f'{graph_name}.pt'\n",
    "        graph.to_pt(graph_path)\n",
    "\n",
    "        slug = graph_name.replace('_', '-')\n",
    "        create_graph_files(\n",
    "            graph_or_path=graph_path,  # the graph to create files for\n",
    "            slug=slug,\n",
    "            output_path=graph_file_dir,\n",
    "            node_threshold=node_threshold,\n",
    "            edge_threshold=edge_threshold\n",
    "        )\n",
    "\n",
    "        port = 8046 + i\n",
    "        server = serve(data_dir='./graph_files/', port=port)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
